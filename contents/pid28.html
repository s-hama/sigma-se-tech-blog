<div class="post-body">
  <h2 id="目的">目的</h2>
  <p>この記事では、ニューラルネットワークの学習に関する基本的な概念を処理の流れに沿ってまとめる。</p>
  <h2 id="学習アルゴリズムの流れまとめ">学習アルゴリズムの流れ（まとめ）</h2>
  <p>ニューラルネットワークの学習とは、<strong>訓練データを基に意図した結果となる最適な重みパラメータを判別する</strong>処理を指す。</p>
  <p>その処理を大きく分類すると、以下の手順（＊1）～（＊4）の4つに分かれ、これを繰り返すことで限りなく<strong>最適な重みパラメータ</strong>に近づけていく。</p>
  <p>
    その結果、<strong>最適な重みパラメータ</strong>によって、<strong>正解率がほぼ100%となる画像認識</strong>や<strong>膨大なデータに基づく根拠ある推測</strong>が実現できるようになる。
  </p>
  <ul>
    <li>
      <p>（＊1）訓練データの抽出（ミニバッチの決定）<br>
        訓練データの中からランダムに<strong>100件程度</strong>（ある程度信頼性がある件数）抽出した<strong>データ群</strong>を<strong>ミニバッチ</strong>といい、以降の（＊2）～（＊4）では、この<strong>ミニバッチ</strong>単位に<strong>損失関数の結果</strong>と<strong>勾配</strong>を求めるアルゴリズムになっている。
      </p>
      <p>※ 参考<br>
        <a href="https://sigma-se.com/detail/23/#:~:text=%E3%81%A8%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB-,%E3%83%9F%E3%83%8B%E3%83%90%E3%83%83%E3%83%81%E5%AD%A6%E7%BF%92%E3%81%A8%E3%81%AF,-%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%A7%E3%81%AF" class="link-secondary">Python - AI : 交差エントロピー誤差のミニバッチ学習と実装サンプル &gt; ミニバッチ学習とは</a>
      </p>
    </li>
    <li>
      <p>（＊2）損失関数の結果取得<br>
        ニューラルネットワークの学習では、<strong>損失関数</strong>という<strong>ニューラルネットワークの性能の悪さ表す指標</strong>を基準にする。</p>
      <p>
        <strong>損失関数</strong>の有名どころでは<strong>2乗和誤差</strong>と<strong>交差エントロピー誤差</strong>があるが、このシリーズでは、<strong>交差エントロピー誤差</strong>にスポットを当てた実装サンプルを記載している。
      </p>
      <p>※ 参考<br></p>
      <ul>
        <li><a href="https://sigma-se.com/detail/22/#:~:text=%E3%81%A3%E3%81%A6%E8%A7%A3%E8%AA%AC%E3%81%99%E3%82%8B%E3%80%82-,2%E4%B9%97%E5%92%8C%E8%AA%A4%E5%B7%AE%E3%81%AE%E5%AE%9A%E7%BE%A9,-2%E4%B9%97%E5%92%8C" class="link-secondary">Python - AI : 損失関数（2乗和誤差、交差エントロピー誤差）と実装サンプル &gt; 2乗和誤差の定義</a></li>
        <li><a href="https://sigma-se.com/detail/22/#:~:text=%EF%BC%89-,2%E4%B9%97%E5%92%8C%E8%AA%A4%E5%B7%AE%E3%81%AE%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB,-Python%20%2D%20AI%20%3A%20MNIST" class="link-secondary">Python - AI : 損失関数（2乗和誤差、交差エントロピー誤差）と実装サンプル &gt; 2乗和誤差の実装サンプル</a></li>
        <li><a href="https://sigma-se.com/detail/23/#:~:text=%E4%BA%A4%E5%B7%AE%E3%82%A8%E3%83%B3%E3%83%88%E3%83%AD%E3%83%94%E3%83%BC%E8%AA%A4%E5%B7%AE%E3%81%AE%E3%83%9F%E3%83%8B%E3%83%90%E3%83%83%E3%83%81%E5%AD%A6%E7%BF%92%EF%BC%88Python%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%EF%BC%89" class="link-secondary">Python - AI : 交差エントロピー誤差のミニバッチ学習と実装サンプル &gt; 交差エントロピー誤差のミニバッチ学習（Python実装サンプル）</a></li>
      </ul>
    </li>
    <li>
      <p>（＊3）勾配の取得<br>
        <strong>損失関数</strong>の結果が<strong>最も小さい値となる重み</strong>となるように<strong>自己探索</strong>（最適な重みパラメータを探す）していくことになる。
      </p>
      <p>
        <strong>損失関数</strong>の結果は、小さいほど正解に近づいているわけだが、もちろんそこで終わりではなく<strong>今の結果より正解に近い</strong>パラメータ候補（重み、バイアス）を決めてさらに正解に近づけていく必要がある。
      </p>
      <p>そこで基準になるのが<strong>重みパラメータの微分結果</strong>（勾配値）。</p>
      <p>※ 参考<br></p>
      <ul>
        <li><a href="https://sigma-se.com/detail/24/#:~:text=%E3%81%A8%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB-,%E6%90%8D%E5%A4%B1%E9%96%A2%E6%95%B0%E3%81%A8%E5%BE%AE%E5%88%86%E3%81%AE%E9%96%A2%E4%BF%82,-%E5%89%8D%E3%81%AE%E8%A8%98%E4%BA%8B" class="link-secondary">Python - AI : 損失関数と数値微分（勾配）の実装サンプル &gt; 損失関数と微分の関係</a></li>
        <li><a href="https://sigma-se.com/detail/24/#:~:text=%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%80%82-,%E5%BE%AE%E5%88%86%E3%81%AE%E3%81%8A%E3%81%95%E3%82%89%E3%81%84,-%E5%BE%AE%E5%88%86%E3%81%A8%E3%81%AF" class="link-secondary">Python - AI : 損失関数と数値微分（勾配）の実装サンプル &gt; 微分のおさらい</a></li>
        <li><a href="https://sigma-se.com/detail/24/#:~:text=%E6%B1%82%E3%82%81%E3%81%A6%E3%81%84%E3%82%8B%E3%80%82-,%E6%95%B0%E5%80%A4%E5%BE%AE%E5%88%86%E3%81%AE%E9%96%A2%E6%95%B0%E5%AE%9A%E7%BE%A9%EF%BC%88Python%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%EF%BC%89,-%E4%B8%8A%E8%A8%98%E3%81%A7%E8%A7%A6%E3%82%8C" class="link-secondary">Python - AI : 損失関数と数値微分（勾配）の実装サンプル &gt; 数値微分の関数定義（Python実装サンプル）</a></li>
        <li><a href="https://sigma-se.com/detail/24/#:~:text=%E6%95%B0%E5%80%A4%E5%BE%AE%E5%88%86%E3%81%AE%E4%BE%8B%EF%BC%88Python%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%EF%BC%89" class="link-secondary">Python - AI : 損失関数と数値微分（勾配）の実装サンプル &gt; 数値微分の例（Python実装サンプル）</a></li>
        <li><a href="https://sigma-se.com/detail/25/#:~:text=%E3%81%A8%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB-,%E5%81%8F%E5%BE%AE%E5%88%86%E3%81%AE%E3%81%8A%E3%81%95%E3%82%89%E3%81%84,-%E5%A4%A7%E3%81%BE%E3%81%8B%E3%81%AB%E8%A8%80%E3%81%86" class="link-secondary">Python - AI : 偏微分と勾配の実装サンプル &gt; 偏微分のおさらい</a></li>
        <li><a href="https://sigma-se.com/detail/25/#:~:text=%E3%81%AE%E3%81%8A%E3%81%95%E3%82%89%E3%81%84-,%E5%81%8F%E5%BE%AE%E5%88%86%E3%81%AEPython%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB,-%E4%BB%A5%E4%B8%8B%E3%80%81" class="link-secondary">Python - AI : 偏微分と勾配の実装サンプル &gt; 偏微分のPython実装サンプル</a></li>
        <li><a href="https://sigma-se.com/detail/25/#:~:text=%E3%81%AB%E9%81%8E%E3%81%8E%E3%81%AA%E3%81%84%E3%80%82-,%E5%8B%BE%E9%85%8D%E3%81%AEPython%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB,-%E4%B8%8A%E8%A8%98%E3%81%A7%E3%80%81" class="link-secondary">Python - AI : 偏微分と勾配の実装サンプル &gt; 勾配のPython実装サンプル</a></li>
      </ul>
    </li>
    <li>
      <p>（＊4）重みパラメータの更新<br>
        重みパラメータを<strong>勾配方向</strong>（より損失が少ない重みパラメータへ）へ微小量だけ更新する。</p>
      <p>※ 参考<br></p>
      <ul>
        <li><a href="https://sigma-se.com/detail/27/#:~:text=%E9%96%A2%E6%95%B0%EF%BC%89%E3%82%92%E8%BF%94%E3%81%99%E3%80%82-,%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%81%AE%E5%AE%9F%E8%A1%8C%E7%A2%BA%E8%AA%8D,-%E4%BB%A5%E9%99%8D%E3%80%81%E4%B8%8A%E8%A8%98simpleNet" class="link-secondary">Python - AI : 重みに対する損失関数の勾配法と実装サンプル &gt; 実装サンプルの実行確認</a></li>
      </ul>
    </li>
  </ul>
  <h2 id="勾配法についての補足">勾配法についての補足</h2>
  <p>上記の<strong>勾配</strong>に関する記述は、すべて<strong>勾配降下法</strong>によってパラメータを更新する方法となる。</p>
  <p>そして<strong>ミニバッチ</strong>は、ランダム抽出したデータ群であることから<strong>確率的勾配降下法</strong>といい、ディープラーニングのフレームワークでは、一般的に<strong>SGD</strong>（stochastic gradient descent）と呼ばれる。</p>
  <p>※ 参考<br></p>
  <ul>
    <li><a
        href="https://sigma-se.com/detail/26/#:~:text=%E3%81%A8%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB-,%E5%8B%BE%E9%85%8D%E6%B3%95%E3%81%A8%E3%81%AF,-%E4%B8%8B%E8%A8%98%E3%81%AE%E8%A8%98%E4%BA%8B" class="link-secondary">Python
        - AI : 勾配降下法の実装サンプル &gt; 勾配法とは</a></li>
    <li><a
        href="https://sigma-se.com/detail/26/#:~:text=%E3%81%A8%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB-,%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95%E3%81%AEPython%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB,-Python%20%2D%20AI%20%3A%20%E5%81%8F%E5%BE%AE%E5%88%86" class="link-secondary">Python
        - AI : 勾配降下法の実装サンプル &gt; 勾配降下法のPython実装サンプル</a></li>
    <li><a
        href="https://sigma-se.com/detail/26/#:~:text=%E3%82%92%E7%B9%B0%E3%82%8A%E8%BF%94%E3%81%99%E5%9B%9E%E6%95%B0-,%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95%E3%81%AE%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E5%AE%9F%E8%A1%8C%E4%BE%8B,-%E6%9C%80%E5%BE%8C%E3%81%AB%E5%AE%9F%E8%A1%8C" class="link-secondary">Python
        - AI : 勾配降下法の実装サンプル &gt; 勾配降下法の実装サンプル実行例</a></li>
  </ul>
  <h3 id="参考文献">参考文献</h3>
  <ul>
    <li>斎藤 康毅（\(2018\)）『ゼロから作るDeep Learning - Pythonで学ぶディープラーニングの理論と実装』株式会社オライリー・ジャパン</li>
  </ul>
</div>
