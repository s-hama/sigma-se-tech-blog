## タイトル
Python - AI : 勾配降下法の実装サンプル

## 目的
この記事では、勾配降下法についての実装サンプルを記載する。

## 概念の説明と実装サンプル
### 勾配法とは
下記の記事で**数値微分による勾配**と**偏微分による勾配**について解説したが、これらの**勾配**では、ある点を基準に実施した**勾配が示す方向**が分かっても**どの程度その方向に**進むと最小値（損失関数の結果が最小）に近づくかまで分からない。
- [Python - AI : 損失関数と数値微分（勾配）の実装サンプル](https://sigma-se.com/detail/24/)
- [Python - AI : 偏微分と勾配の実装サンプル](https://sigma-se.com/detail/25/)

そこでより正確な**最小値**を求めるために、この**勾配**を複数回実施して、その勾配が示す方向に基準をずらしながら**損失関数の結果が最小となる値**を探していく。

これを**勾配法**といい、最小値を探す**勾配降下法**と最大値を探す**勾配上昇法**に分類される。<br>
※ ニューラルネットワークの学習では、**勾配降下法**が多く用いられている。

ニューラルネットワークでも最適な**重み**や**バイアス**を学習時に決定するため、**勾配法**を用いて**損失関数の結果が最小となる値**を算出していくことになる。

変数が \\(x_{0}\\) と \\(x_{1}\\) の2つである場合を例に**勾配降下法**を数式で表現すると

<div style="display: flex; margin-left: 1rem; font-size: 1.2em; margin-top: -0.75em; overflow-x: auto; white-space: nowrap;">
\[
 x_{0} = x_{0} - μ\frac{∂\ (x_{0}, x_{1})}{∂x_{0}}\hspace{5mm}･･･（A）
\]
</div>
<div style="display: flex; margin-left: 1rem; font-size: 1.2em; margin-top: -0.75em; overflow-x: auto; white-space: nowrap;">
\[
 x_{1} = x_{1} - μ\frac{∂\ (x_{0}, x_{1})}{∂x_{1}}\hspace{5mm}･･･（B）
\]
</div>

と表せ、**\\(μ\\)** を**学習率**といい、複数回の勾配を実施するにあたって前回の勾配が示す方向へどのくらい進めるかの**基準値**となる。<br>
（勾配1回に対し、\\(（A）\\)と\\(（B）\\)をそれぞれ一回実施する。）

※ \\(\displaystyle \frac{∂\ (x_{0}, x_{1})}{∂x_{0}}\\) と \\(\displaystyle \frac{∂\ (x_{0}, x_{1})}{∂x_{1}}\\) の**偏微分**については、[Python - AI : 偏微分と勾配の実装サンプル > 勾配のPython実装サンプル](https://sigma-se.com/detail/25/#:~:text=%E3%81%AB%E9%81%8E%E3%81%8E%E3%81%AA%E3%81%84%E3%80%82-,%E5%8B%BE%E9%85%8D%E3%81%AEPython%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB,-%E4%B8%8A%E8%A8%98%E3%81%A7%E3%80%81) を参考のこと。

- **学習率**についての補足<br>
下記の記事で触れている**重み**は、機械学習アルゴリズムにより自動で変化していくが、**学習率**は、あらかじめ人の手で設定するパラメータとなり、深層学習では勾配法によって、**最適化できない（しない）**パラメータになる。<br>
（このようなパラメータを**ハイパーパラメータ**という。）
  - [Python - AI : 単純パーセプトロンの概念と実装サンプル](https://sigma-se.com/detail/15/)
  - [Python - AI : 多層パーセプトロンの概念と実装サンプル](https://sigma-se.com/detail/16/)
  - [Python - AI : ニューラルネットワークの活性化関数と実装サンプル](https://sigma-se.com/detail/17/)


### 勾配降下法のPython実装サンプル
[Python - AI : 偏微分と勾配の実装サンプル > 勾配のPython実装サンプル](https://sigma-se.com/detail/25/#:~:text=%E3%81%AB%E9%81%8E%E3%81%8E%E3%81%AA%E3%81%84%E3%80%82-,%E5%8B%BE%E9%85%8D%E3%81%AEPython%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB,-%E4%B8%8A%E8%A8%98%E3%81%A7%E3%80%81) で解説した**勾配関数**（num_gradient）を使用する。

- **勾配関数**（num_gradient）
    ```python
    $ python
        >>> import numpy as np
        >>>
        >>> def num_gradient(f,x):    # 勾配関数
        ...     h = 1e-4
        ...     grad = np.zeros_like(x)    # xと同じ形状の配列で値がすべて 0
        ...
        ...     for idx in range(x.size):    # x の次元分ループする。 (下記例は、5.0, 10.0 の 2 周ループ)
        ...         idx_val = x[idx]
        ...         x[idx] = idx_val + h    # f(x + h)の算出。
        ...         fxh1 = f(x)
        ...
        ...         x[idx] = idx_val - h    # f(x - h)の算出。
        ...         fxh2 = f(x)
        ...
        ...         grad[idx] = (fxh1 - fxh2) / (2 * h)
        ...         x[idx] = idx_val    # 値をループ先頭の状態に戻す。
        ...     return grad
        ...
        >>>
    ```

- 勾配法の実装サンプル<br>
    この**勾配関数**（num_gradient）をラップした形の**勾配法**となる上記\\(（A）\\)と\\(（B）\\)の実装サンプル。
    ```python
        >>> # 上記対話モードの続き
        >>> def gradient_descent(f, init_x, lr=0.01, step_num=100):
        ...     x = init_x
        ...
        ...     for i in range(step_num):
        ...         grad = num_gradient(f, x)    # 上記 勾配関数「num_gradient」をコール
        ...         x -= lr * grad    # (A)、(B) の実装箇所
        ...
        ...     return x
        ...
        >>>
    ```
    ※ `gradient_descent`の引数
    - 第1引数「f」：最適化したい関数
    - 第2引数「init_x」：初期値
    - 第3引数「lr」：学習率
    - 第4引数「step_num」：勾配を繰り返す回数

### 勾配降下法の実装サンプル実行例
最後に実行サンプル。
- 勾配法の実行例<br>
    関数 \\(f(x_{0}, x_{1}) = x^2_{0} + x^2_{1}\\) で \\(x_{0} = -15.0\\) 、\\(x_{1} = 20\\) とした時の勾配法`gradient_descent`の実行例。
    ```python
        >>> # ↑↑↑ 上記対話モードの続き
        >>> def func_ex(x):
        ...     return x[0]**2 + x[1]**2
        ...
        >>> init_x = np.array([-15.0, 20.0])
        >>>
        >>> gradient_descent(func_ex, init_x=init_x, lr=0.1, step_num=100)
        array([-3.05555396e-09,  4.07407195e-09])
        >>>
    ```
    ※ `gradient_descent`の引数
    - 関数「f」：\\(f(x_{0}, x_{1})\\) \\(= x^2_{0} + x^2_{1}\\)
    - 初期値「init_x」： \\(x_{0} = -15.0\\) 、\\(x_{1} = 20\\)
    - 学習率「lr」：0.1
    - 勾配繰返「step_num」：100

    初期値 **init_x**：\\(x_{0} = -15.0\\) 、\\(x_{1} = 20\\) の実行結果は、\\(x_{0}\\) \\(= -3.05555396e-09\\) 、\\(x_{1}\\) \\(= 4.07407195e-09\\) となり、真の最小値が \\(x_{0} = 0\\) 、\\(x_{1} = 0\\) なのでほぼ正しい結果となっていることが分かる。

    ※ \\(e\\)について<br>
    -3.05555396e-09 ≒  -3.1 * 10のマイナス9乗 ≒ -0.0000000031
    4.07407195e-09 ≒  4.1 * 10のマイナス9乗 ≒  0.0000000041

    試しに、学習率を**極端に増減**させてみると
    ```python
        >>> # 上記対話モードの続き
        >>> gradient_descent(func_ex, init_x=init_x, lr=10, step_num=100)
        array([ 3.10343509e+12, -8.05258867e+12])
        >>>
        >>> gradient_descent(func_ex, init_x=init_x, lr=1e-10, step_num=100)
        array([-14.9999997,  19.9999996])
        >>>
    ```
    学習率を \\(10\\) とした場合、\\(x_{0}\\) \\(= 3.10343509e+12\\) 、\\(x_{1}\\) \\(= -8.05258867e+12\\) となりプラス方向に大きく発散している。

    これは、学習率が大きすぎることを意味しており、反対に学習率を \\(1e-10\\) と \\(0\\) に近づけすぎると \\(x_{0} = -14.9999997\\) 、\\(x_{1} = 19.9999996\\) となり初期値とほぼ同じ値が出力され、学習率が小さすぎることを意味している。

    実際のニューラルネットワークの学習においても、以上のように妥当な**学習率**を事前に決め、**勾配**を繰り返し、真の最小値を求めていくことになる。

### 参考文献
- 斎藤 康毅（2018）『ゼロから作るDeep Learning - Pythonで学ぶディープラーニングの理論と実装』株式会社オライリー・ジャパン
