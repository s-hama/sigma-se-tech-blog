## タイトル
Python - AI : 重みに対する損失関数の勾配法と実装サンプル

## 目的
この記事では、重みに対する勾配法（勾配降下法）の実装サンプルを記載する。

## 概念の説明と実装サンプル

### 重みに対する勾配法とは

**重み**は、正解に対して**入力値**がどれだけ影響するかを示す**重要度**を表す。

**重み**という言葉からは全くイメージできない意味を持つが、これは英語の**Weigh**を直訳し、**重み**となっているからであり、本来は**大切さ**、**価値**、**重要性**という意味で命名されている。

そして、この**重み**（重要度）は、一般的に**Weigh**の \\(w\\) を取り、\\(w_{0}\\)、\\(w_{1}\\)、\\(w_{2}\\) …で表現され、**評価的に具体的な数値を入れてみて損失結果を計る**ためのパラメータとなる。

ニューラルネットワークでは、この**重み**に対して、[Python - AI : 勾配降下法の実装サンプル](https://sigma-se.com/detail/26/) で解説した**勾配降下法**を実施し、最適な**重み**を求めていく。

また、下記に示す行列のよう色々なパターンの**重み**を行列計算で一斉に**偏微分**し、最適な**重み**を求めていく。

<div style="display: flex; margin-left: 1rem; font-size: 1.2em; margin-top: -0.75em; overflow-x: auto; white-space: nowrap;">
\[
W =
\begin{pmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
\end{pmatrix}
\]
</div>

<div style="display: flex; margin-left: 1rem; font-size: 1.2em; margin-top: -0.75em; overflow-x: auto; white-space: nowrap;">
\[
\frac{∂L}{∂W} =
\begin{pmatrix}
\frac{∂L}{∂w_{11}} & \frac{∂L}{∂w_{12}} & \frac{∂L}{∂w_{13}} \\
\frac{∂L}{∂w_{21}} & \frac{∂L}{∂w_{22}} & \frac{∂L}{∂w_{23}} \\
\end{pmatrix}
\]
</div>

\\(W\\) は、一斉に偏微分しようとしている2行3列の**重み**達。<br>
\\(L\\) は、対象となる**損失関数**で \\(\displaystyle \frac{∂L}{∂W}\\) の各要素で偏微分し、**損失関数**\\(L\\) の**勾配**を求めている。

以降は、この**勾配**を求める実装サンプルについて記載する。
