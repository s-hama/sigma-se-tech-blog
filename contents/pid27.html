<div class="post-body">
  <h2 id="目的">目的</h2>
  <p>この記事では、重みに対する勾配法（勾配降下法）の実装サンプルを記載する。</p>
  <h2 id="概念の説明と実装サンプル">概念の説明と実装サンプル</h2>
  <h3 id="重みに対する勾配法とは">重みに対する勾配法とは</h3>
  <p><strong>重み</strong>は、正解に対して<strong>入力値</strong>がどれだけ影響するかを示す<strong>重要度</strong>を表す。</p>
  <p>
    <strong>重み</strong>という言葉からは全くイメージできない意味を持つが、これは英語の<strong>Weigh</strong>を直訳し、<strong>重み</strong>となっているからであり、本来は<strong>大切さ</strong>、<strong>価値</strong>、<strong>重要性</strong>という意味で命名されている。
  </p>
  <p>そして、この<strong>重み</strong>（重要度）は、一般的に<strong>Weigh</strong>の \(w\) を取り、\(w_{0}\)、\(w_{1}\)、\(w_{2}\)
    …で表現され、<strong>評価的に具体的な数値を入れてみて損失結果を計る</strong>ためのパラメータとなる。</p>
  <p>ニューラルネットワークでは、この<strong>重み</strong>に対して、<a href="https://sigma-se.com/detail/26/" class="link-secondary">Python - AI : 勾配降下法の実装サンプル</a>
    で解説した<strong>勾配降下法</strong>を実施し、最適な<strong>重み</strong>を求めていく。</p>
  <p>また、下記に示す行列のよう色々なパターンの<strong>重み</strong>を行列計算で一斉に<strong>偏微分</strong>し、最適な<strong>重み</strong>を求めていく。</p>
  <div
    style="display: flex; margin-left: 1rem; font-size: 1.2em; margin-top: -0.75em; overflow-x: auto; white-space: nowrap;">
    \[
    W =
    \begin{pmatrix}
    w_{11} & w_{12} & w_{13} \\
    w_{21} & w_{22} & w_{23} \\
    \end{pmatrix}
    \]
  </div>
  <div
    style="display: flex; margin-left: 1rem; font-size: 1.2em; margin-top: -0.75em; overflow-x: auto; white-space: nowrap;">
    \[
    \frac{∂L}{∂W} =
    \begin{pmatrix}
    \frac{∂L}{∂w_{11}} & \frac{∂L}{∂w_{12}} & \frac{∂L}{∂w_{13}} \\
    \frac{∂L}{∂w_{21}} & \frac{∂L}{∂w_{22}} & \frac{∂L}{∂w_{23}} \\
    \end{pmatrix}
    \]
  </div>
  <p>\(W\) は、一斉に偏微分しようとしている2行3列の<strong>重み</strong>達。<br>
    \(L\) は、対象となる<strong>損失関数</strong>で \(\displaystyle \frac{∂L}{∂W}\) の各要素で偏微分し、<strong>損失関数</strong>\(L\)
    の<strong>勾配</strong>を求めている。</p>
  <p>以降は、この<strong>勾配</strong>を求める実装サンプルについて記載する。</p>
  <h3 id="重みに対する勾配のpython実装サンプル">重みに対する勾配のPython実装サンプル</h3>
  <p>以下、参考文献『ゼロから作るDeep Learning』から提供されている <code>ch04/gradient_simplenet.py</code> を用いたサンプル解説をしていく。</p>
  <p>※ サンプルコードは、下記Gitからダウンロードする。<br>
    Git(deep-learning-from-scratch)：
    <a
      href="https://github.com/oreilly-japan/deep-learning-from-scratch" class="link-secondary">https://github.com/oreilly-japan/deep-learning-from-scratch</a>
  </p>
  <p>
    ここでは、より分かりやすく説明するため、<code>ch04/gradient_simplenet.py</code>の<code>simpleNet</code>クラスを実装するにあたりimportされている下記3つの関数をあえてPython対話モードで定義する形で記載する。
  </p>
  <ul>
    <li>
      <p>ソフトマックス関数：common/functions.pyのsoftmax関数<br>
        ※ ソフトマックス関数の一般的な定義は下記ページを参考。<br>
        <a
          href="https://sigma-se.com/detail/18/#:~:text=pid18_4.png%27)%0A%20%3E%3E%3E-,%E3%82%BD%E3%83%95%E3%83%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E9%96%A2%E6%95%B0,-%E5%88%86%E9%A1%9E%E5%95%8F%E9%A1%8C%E3%81%A7" class="link-secondary">Python
          - AI : 活性化関数の実装サンプルまとめ（ステップ、シグモイド、ReLU、恒等関数、ソフトマックス関数） &gt; ソフトマックス関数</a>
      </p>
      <pre><code class="language-python">$ python
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">x</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> x.ndim == <span class="hljs-number">2</span>:
<span class="hljs-meta">... </span>        x = x.T
<span class="hljs-meta">... </span>        x = x - np.<span class="hljs-built_in">max</span>(x, axis=<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>        y = np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x), axis=<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> y.T
<span class="hljs-meta">... </span>    x = x - np.<span class="hljs-built_in">max</span>(x)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> np.exp(x) / np.<span class="hljs-built_in">sum</span>(np.exp(x))
...
&gt;&gt;&gt;
</code></pre>
    </li>
    <li>
      <p>交差エントロピー誤差：common/functions.py の cross_entropy_error関数<br>
        ※ 交差エントロピー誤差の処理内容については、下記ページを参考。<br>
        <a
          href="https://sigma-se.com/detail/22/#:~:text=%EF%BC%89-,%E4%BA%A4%E5%B7%AE%E3%82%A8%E3%83%B3%E3%83%88%E3%83%AD%E3%83%94%E3%83%BC%E8%AA%A4%E5%B7%AE%E3%81%A8%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB,-%E5%89%8D%E9%A0%85%E3%81%A8%E5%90%8C%E6%A7%98" class="link-secondary">Python
          - AI : 損失関数（2乗和誤差、交差エントロピー誤差）と実装サンプル &gt; ソフトマックス関数</a>
        <a
          href="https://sigma-se.com/detail/23/#:~:text=%E4%BA%A4%E5%B7%AE%E3%82%A8%E3%83%B3%E3%83%88%E3%83%AD%E3%83%94%E3%83%BC%E8%AA%A4%E5%B7%AE%E3%81%AE%E3%83%9F%E3%83%8B%E3%83%90%E3%83%83%E3%83%81%E5%AD%A6%E7%BF%92%EF%BC%88Python%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%EF%BC%89" class="link-secondary">Python
          - AI : 交差エントロピー誤差のミニバッチ学習と実装サンプル</a>
      </p>
      <pre><code class="language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 上記対話モードの続き</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">cross_entropy_error</span>(<span class="hljs-params">y, t</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> y.ndim == <span class="hljs-number">1</span>:
<span class="hljs-meta">... </span>        t = t.reshape(<span class="hljs-number">1</span>, t.size)
<span class="hljs-meta">... </span>        y = y.reshape(<span class="hljs-number">1</span>, y.size)
...
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> t.size == y.size:
<span class="hljs-meta">... </span>        t = t.argmax(axis=<span class="hljs-number">1</span>)
...
<span class="hljs-meta">... </span>    batch_size = y.shape[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> -np.<span class="hljs-built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="hljs-number">1e-7</span>)) / batch_size
...
&gt;&gt;&gt;
</code></pre>
    </li>
    <li>
      <p>勾配処理：common/gradient.pyのnumerical_gradient関数<br>
        ※ 勾配の処理内容は下記ページの勾配関数(num_gradient)を参考。<br>
        <a
          href="https://sigma-se.com/detail/25/#:~:text=%E3%81%AB%E9%81%8E%E3%81%8E%E3%81%AA%E3%81%84%E3%80%82-,%E5%8B%BE%E9%85%8D%E3%81%AEPython%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB,-%E4%B8%8A%E8%A8%98%E3%81%A7%E3%80%81" class="link-secondary">Python
          - AI : 偏微分と勾配の実装サンプル</a>
      </p>
      <pre><code class="language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 上記対話モードの続き</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">numerical_gradient</span>(<span class="hljs-params">f, x</span>):
<span class="hljs-meta">... </span>    h = <span class="hljs-number">1e-4</span>
<span class="hljs-meta">... </span>    grad = np.zeros_like(x)
...
<span class="hljs-meta">... </span>    it = np.nditer(x, flags=[<span class="hljs-string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="hljs-string">&#x27;readwrite&#x27;</span>])
...
<span class="hljs-meta">... </span>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> it.finished:
<span class="hljs-meta">... </span>        idx = it.multi_index
<span class="hljs-meta">... </span>        tmp_val = x[idx]
<span class="hljs-meta">... </span>        x[idx] = <span class="hljs-built_in">float</span>(tmp_val) + h
<span class="hljs-meta">... </span>        fxh1 = f(x)
...
<span class="hljs-meta">... </span>        x[idx] = tmp_val - h
<span class="hljs-meta">... </span>        fxh2 = f(x)
<span class="hljs-meta">... </span>        grad[idx] = (fxh1 - fxh2) / (<span class="hljs-number">2</span>*h)
...
<span class="hljs-meta">... </span>        x[idx] = tmp_val
<span class="hljs-meta">... </span>        it.iternext()
...
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> grad
...
&gt;&gt;&gt;
</code></pre>
    </li>
    <li>
      <p>重みに対する勾配処理：3つの関数を呼び出した形で<code>ch04/gradient_simplenet.py</code>の<code>simpleNet</code>クラスを実装</p>
      <pre><code class="language-python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 上記対話モードの続き</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> sys, os
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">class</span> <span class="hljs-title class_">simpleNet</span>:
<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):
<span class="hljs-meta">... </span>        self.W = np.random.randn(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)
...
<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x</span>):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> np.dot(x, self.W)
...
<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss</span>(<span class="hljs-params">self, x, t</span>):
<span class="hljs-meta">... </span>        z = self.predict(x)
<span class="hljs-meta">... </span>        y = softmax(z)
<span class="hljs-meta">... </span>        loss = cross_entropy_error(y, t)
...
<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> loss
...
&gt;&gt;&gt;
</code></pre>
      <p>\(x\) は、<strong>入力データ</strong>で \(t\) が<strong>教師データ</strong>。</p>
      <p>predict関数は、入力データ \(x\)
        と<code>__init__</code>で設定した仮（ランダム）の重みパラメータ<code>self.W</code>の<strong>評価結果</strong>（積）を返す。</p>
      <p>loss関数は、predict関数、softmax関数を実施した \(y\) と教師データ \(t\)
        の<strong>損失関数</strong>（交差エントロピー誤差：cross_entropy_error関数）を返す。</p>
    </li>
  </ul>
</div>
